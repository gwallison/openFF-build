# -*- coding: utf-8 -*-
"""
Created on Sat Oct 30 12:57:54 2021

@author: Gary

Used to create a full set of data, code and documentation that can be
the source of any use of the Open-FF products.  It is meant to
create an anchor that anyone working on the data can refer to.

Provided in the repo are: 
    - data table pickles (for recreating analysis sets)
    - zips of filtered and full sets
    - copies of the translation tables used to created the database
    - a readme file that explains things like when the data were downloaded,
      when the data were compiled, what code version was used to create it,
      etc.
    - maybe a simple script to help user extract their own dataset from the 
      pickles.
    - a csv file of hashes for most of the files in the repo.  These can 
      be used to verify that the files have not been changed since creation.
"""

import os, shutil
from pathlib import Path
from hashlib import sha256
import pandas as pd
import core.Analysis_set as ana_set
import datetime
import zipfile
import build_common
outdir = build_common.get_pickle_dir()
sources = build_common.get_data_dir()
trans_dir = build_common.get_transformed_dir()
tempfolder = './tmp/'


data_source = 'SkyTruth'  # 'bulk' for typical processing
repo_name = 'SkyTruth_2022_06_08'
#repo_name = 'junk'

q = input(f'Data source is set to < {data_source} > and repo to {repo_name}. \n\n -- Enter "y" to start building process.  > ')
assert q=='y'

repo_dir = build_common.get_repo_dir() + repo_name
pklsource = 'currentData_pickles'

descriptive_notes = f""" This is an OpenFF data repository for the 
bulk download of FracFocus from May 28, 2022.

Created {datetime.date.today()}
CodeOcean version: 14 with changes:
    - added a criteria to MI_inconsistent, which finds an additional 800+
      inconsistent disclosures.
    - includes revamp of location data including checks against shapefiles.
"""


descriptive_notes = """ This is an OpenFF data repository for an
auxilary data set: the archive created by SkyTruth from scraping PDFs
in 2011 through May 2013.  We are aware that there are some differences
between this set of data and the current version of these disclosures; 
this is likely due to companies changing disclosures after their
publication - a behavior that is allowed by FracFocus but does not provide
either justification or even notification.

Created 2022-06-08
CodeOcean version: 14 as released with changes.
    - includes revamp of location data including checks against shapefiles.

This directory contains a data set generated by the Open-FF
project.
"""

boilerplate = """This directory contains a data set generated by the Open-FF
project.
"""

print(f'Starting creation of new Data Repo set: {repo_name}')
# create new directory
try:
    os.mkdir(repo_dir)
except:
    print(f'\nCreation of Directory <{repo_dir}> not allowed;  already created?')

# create and store README
with open(repo_dir+'/README.txt','w') as f:
    f.write(descriptive_notes+'\n')
    f.write(boilerplate)  # see below for the text

# # generate output csv's 
# first try to delete any existing big files
try:
    os.remove(repo_dir+'/standard_filtered.zip')
    print('removed old std_filtered')
except:
    pass
try:
    os.remove(repo_dir+'/full_no_filter.zip')
    print('removed old full')
except:
    pass
try:
    os.remove(repo_dir+'/catalog_set.zip')
    print('removed old catalog_set')
except:
    pass


# now make the zipped sets
ana_set.Standard_data_set().save_compressed()
shutil.move(outdir+'standard_filtered.zip',repo_dir+'/standard_filtered.zip')

ana_set.Full_set().save_compressed()
shutil.move(outdir+'full_no_filter.zip',repo_dir+'/full_no_filter.zip')

ana_set.Catalog_set().save_compressed()
shutil.move(outdir+'catalog_set.zip',repo_dir+'/catalog_set.zip')


# copy pickles
pickledir = repo_dir+'/pickles'
try:
    os.mkdir(pickledir)
except:
    print(f'\nDirectory <{pickledir}> not created;  already created?')
flst = os.listdir(outdir+pklsource)
for fn in flst:
    if fn[-4:]=='.pkl':
        if not (fn[-7:]=='_df.pkl'):  # ignore pickled analysis sets
            shutil.copyfile(outdir+pklsource+'/'+fn, pickledir+'/'+fn)
            print(f'copied {fn}')
        
# copy curation files

cfiles = ['carrier_list_auto.csv',
          'carrier_list_curated.csv',
          'carrier_list_prob.csv']
files = ['CAS_curated.csv',
         'casing_curated.csv','company_xlate.csv','ST_api_without_pdf.csv',
         'ING_curated.csv','CAS_synonyms.csv',
         'CAS_synonyms_CompTox.csv','CAS_ref_and_names.csv',
         'tripwire_summary.csv','upload_dates.csv']

cdir = 'curation_files/'
os.mkdir(cdir) # made in the cwd.
with zipfile.ZipFile(repo_dir+'/curation_files.zip','w') as z:
    for fn in files:
        print(f'  - zipping {fn}')
        shutil.copy(trans_dir+fn,cdir)
        z.write(cdir+fn,compress_type=zipfile.ZIP_DEFLATED)    
    for fn in cfiles:
        print(f'  - zipping {fn}')
        shutil.copy(trans_dir+f'{data_source}/{fn}',cdir)
        z.write(cdir+fn,compress_type=zipfile.ZIP_DEFLATED)    
shutil.rmtree(cdir)         

# now create hashfile
#  this is a pandas df with all files (except the hashfile) in the "filename"
#  field and the sha256 hash of the file in the "sha256" field.
#  These hashes can be used to verify that the data is in the original state and
#  has not been modified.

print('\nMaking file hashes for validation')

to_hash = ['pickles/bgCAS.pkl',
           'pickles/cas_ing.pkl',
           'pickles/chemrecs.pkl', 
           'pickles/companies.pkl',
           'pickles/disclosures.pkl',
           'curation_files.zip',                                                        
           'README.txt',
           'standard_filtered.zip',
           'full_no_filter.zip',
           'catalog_set.zip']

fnout = []; fnhash = []
for fn in to_hash:
    path = Path(os.path.join(repo_dir,fn))
    if path.is_file():
        print('  -- '+fn)
        fnout.append(fn)
        with open(path,'rb') as f:
            fnhash.append(sha256(f.read()).hexdigest())
    else:
        print(f'  >> file not in repo: {fn} <<')
pd.DataFrame({'filename':fnout,'filehash':fnhash}).to_csv(repo_dir+'/filehash.csv',
                                                          index=False)

print(f'Repo creation completed: {repo_dir}')