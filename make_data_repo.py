# -*- coding: utf-8 -*-
"""
Created on Sat Oct 30 12:57:54 2021

@author: Gary

Used to create a full set of data, code and documentation that can be
the source of any use of the Open-FF products.  It is meant to
create an anchor that anyone working on the data can refer to.

Provided in the repo are: 
    - data table pickles (for recreating analysis sets)
    - zips of filtered and full sets
    - copies of the translation tables used to created the database
    - a readme file that explains things like when the data were downloaded,
      when the data were compiled, what code version was used to create it,
      etc.
    - maybe a simple script to help user extract their own dataset from the 
      pickles.
"""

import os, shutil
import core.Analysis_set as ana_set
import datetime
import zipfile
import common
outdir = common.get_pickle_dir()
sources = common.get_data_dir()
trans_dir = common.get_transformed_dir()
tempfolder = './tmp/'

repo_name = '2021-11-28_testing'
repo_dir = common.get_repo_dir() + repo_name
pklsource = 'currentData_pickles'

descriptive_notes = f""" This is a test data repo.
Created {datetime.date.today()}
Bulk download from FracFocus on:  11-08-2021
CodeOcean version: 10 -- but modified including sets 2-5
Types of changes from CodeOcean version:
"""

boilerplate = """This directory contains a data set generated by the Open-FF
project.
"""

print(f'Starting creation of new Data Repo set: {repo_name}')
# create new directory
try:
    os.mkdir(repo_dir)
except:
    print(f'\nDirectory <{repo_dir}> not created;  already created?')

# create and store README
with open(repo_dir+'/README.txt','w') as f:
    f.write(descriptive_notes+'\n')
    f.write(boilerplate)  # see below for the text

# generate output csv's 
# ana_set.Standard_data_set().save_compressed()
# shutil.move(outdir+'standard_filtered.zip',repo_dir+'/standard_filtered.zip')
# ana_set.Full_set().save_compressed()
# shutil.move(outdir+'full_no_filter.zip',repo_dir+'/full_no_filter.zip')
# ana_set.Catalog_set().save_compressed()
# shutil.move(outdir+'catalog_set.zip',repo_dir+'/catalog_set.zip')

# copy pickles
pickledir = repo_dir+'/pickles'
try:
    os.mkdir(pickledir)
except:
    print(f'\nDirectory <{pickledir}> not created;  already created?')
flst = os.listdir(outdir+pklsource)
for fn in flst:
    if fn[-4:]=='.pkl':
        if not (fn[-7:]=='_df.pkl'):  # ignore pickled analysis sets
            shutil.copyfile(outdir+pklsource+'/'+fn, pickledir+'/'+fn)
            print(f'copied {fn}')
        
# copy curation files
files = ['carrier_list_auto.csv','carrier_list_curated.csv',
         'carrier_list_prob.csv','CAS_curated.csv',
         'casing_curated.csv','company_xlate.csv','ST_api_without_pdf.csv',
         'ING_curated.csv','CAS_synonyms.csv','CAS_ref_and_names.csv']

cdir = 'curation_files/'
os.mkdir(cdir) # made in the cwd.
with zipfile.ZipFile(repo_dir+'/curation_files.zip','w') as z:
    for fn in files:
        print(f'  - zipping {fn}')
        shutil.copy(trans_dir+fn,cdir)
        z.write(cdir+fn,compress_type=zipfile.ZIP_DEFLATED)    
shutil.rmtree(cdir)         


